services:
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    runtime: nvidia
    environment:
      - OLLAMA_HOST=0.0.0.0
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - ollama:/root/.ollama
    ports:
      - "127.0.0.1:11434:11434" # bind to localhost so it's not public
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  api:
    build: .
    restart: unless-stopped
    environment:
      # simple bearer token for your gateway
      - APP_API_KEY=${APP_API_KEY}
      # default model to use if client doesn't specify
      - DEFAULT_MODEL=${DEFAULT_MODEL}
      # always force using DEFAULT_MODEL (ignore client model)
      - ENFORCE_DEFAULT_MODEL=true
      # allow your frontend origin (comma-separated for many)
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS}
      # talk to ollama over the internal docker network
      - OLLAMA_URL=http://ollama:11434
    depends_on:
      - ollama
    ports:
      - "0.0.0.0:3069:3001"

volumes:
  ollama:
