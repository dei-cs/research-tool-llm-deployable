services:
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=0.0.0.0
    volumes:
      - ollama:/root/.ollama
    ports:
      - "127.0.0.1:11434:11434" # bind to localhost so it's not public

  api:
    build: .
    restart: unless-stopped
    environment:
      # simple bearer token for your gateway
      - APP_API_KEY=${APP_API_KEY}
      # default model to use if client doesn't specify
      - DEFAULT_MODEL=${DEFAULT_MODEL}
      # always force using DEFAULT_MODEL (ignore client model)
      - ENFORCE_DEFAULT_MODEL=true
      # allow your frontend origin (comma-separated for many)
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS}
      # talk to ollama over the internal docker network
      - OLLAMA_URL=http://ollama:11434
    depends_on:
      - ollama
    ports:
      - "0.0.0.0:3001:3001"


  # nginx:
  #   image: nginx:alpine
  #   restart: unless-stopped
  #   ports:
  #     - "0.0.0.0:80:80"
  #     - "0.0.0.0:443:443"
  #   volumes:
  #     - ./certs:/etc/nginx/certs:ro
  #     - ./nginx.conf:/etc/nginx/nginx.conf:ro
  #   depends_on:
  #     - api


volumes:
  ollama:
